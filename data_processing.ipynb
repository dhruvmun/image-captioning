{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Model\n",
    "   from keras.applications.imagenet_utils import preprocess_input \n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTION_PATH = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "TRAIN_IMAGES_PATH = 'Flickr8k_text/Flickr_8k.trainImages.txt' \n",
    "VAL_IMAGES_PATH = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "TEST_IMAGES_PATH = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "DATASET = 'Flicker8k_Dataset/Flicker8k_Dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "First we create a dictonary with the image file name as the key and the caption as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .',\n",
       " '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .',\n",
       " '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .',\n",
       " '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = open(CAPTION_PATH,'r').read().strip().split('\\n')\n",
    "captions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary with file name as key and captions as value\n",
    "d = {}\n",
    "for i, row in enumerate(captions):\n",
    "    row = row.split('\\t')\n",
    "    row[0] = row[0][:len(row[0])-2]\n",
    "    if row[0] in d:\n",
    "        d[row[0]].append(row[1])\n",
    "    else:\n",
    "        d[row[0]] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flicker8k_Dataset/2249865945_f432c8e5da.jpg',\n",
       " 'Flicker8k_Dataset/3223302125_f8154417f4.jpg',\n",
       " 'Flicker8k_Dataset/3552796830_2dd2aa9c2c.jpg',\n",
       " 'Flicker8k_Dataset/3230101918_7d81cb0fc8.jpg',\n",
       " 'Flicker8k_Dataset/535399240_0714a6e950.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#img -> Path of all images\n",
    "img = glob.glob(DATASET +'*.jpg')\n",
    "img[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_images -> Filename of train_images\n",
    "#train_img -> Path of train_images\n",
    "f_train_images = open(TRAIN_IMAGES_PATH, 'r')\n",
    "train_images = f_train_images.read().strip().split('\\n')\n",
    "\n",
    "train_img = []\n",
    "for i in train_images:\n",
    "    temp = DATASET + i\n",
    "    train_img.append(temp)\n",
    "\n",
    "f_train_images.close()\n",
    "len(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_val_images = open(VAL_IMAGES_PATH, 'r')\n",
    "val_images = f_val_images.read().strip().split('\\n')\n",
    "\n",
    "val_img = []\n",
    "for i in val_images:\n",
    "    temp = DATASET + i\n",
    "    val_img.append(temp)\n",
    "\n",
    "f_val_images.close()\n",
    "len(val_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_test_images = open(TEST_IMAGES_PATH, 'r')\n",
    "test_images = f_test_images.read().strip().split('\\n')\n",
    "\n",
    "test_img = []\n",
    "for i in test_images:\n",
    "    temp = DATASET + i\n",
    "    test_img.append(temp)\n",
    "\n",
    "f_test_images.close()\n",
    "len(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the dictonary items into different files, each for test, validation and training datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_train_dataset = open('Flickr8k_text/flickr_8k_train_dataset.txt','w')\n",
    "f_train_dataset.write(\"image_id\\tcaptions\\n\")\n",
    "\n",
    "f_val_dataset = open('Flickr8k_text/flickr_8k_val_dataset.txt','w')\n",
    "f_val_dataset.write(\"image_id\\tcaptions\\n\")\n",
    "\n",
    "f_test_dataset = open('Flickr8k_text/flickr_8k_test_dataset.txt','w')\n",
    "f_test_dataset.write(\"image_id\\tcaptions\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train = 0\n",
    "for i in train_images:\n",
    "    for capt in d[i]:\n",
    "        caption = \"<start> \"+ capt +\" <end>\"\n",
    "        f_train_dataset.write(i+\"\\t\"+caption+\"\\n\")\n",
    "        f_train_dataset.flush()\n",
    "        c_train += 1\n",
    "\n",
    "f_train_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_val = 0\n",
    "for i in val_images:\n",
    "    for capt in d[i]:\n",
    "        caption = \"<start> \"+ capt +\" <end>\"\n",
    "        f_val_dataset.write(i+\"\\t\"+caption+\"\\n\")\n",
    "        f_val_dataset.flush()\n",
    "        c_val += 1\n",
    "\n",
    "f_val_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_test = 0\n",
    "for i in test_images:\n",
    "    for capt in d[i]:\n",
    "        caption = \"<start> \"+ capt +\" <end>\"\n",
    "        f_test_dataset.write(i+\"\\t\"+caption+\"\\n\")\n",
    "        f_test_dataset.flush()\n",
    "        c_test += 1\n",
    "\n",
    "f_test_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL:\n",
    "We use an VGG16 Model pretrained on the ImageNet dataset.\n",
    "We remove the last softmax layer of the VGG model, so that we can obtain the features of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights = \"imagenet\", include_top=True, input_shape=(224, 224, 3))\n",
    "new_input = model.input\n",
    "hidden_layer = model.layers[-2].output\n",
    "\n",
    "encoding_model = Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pass all the images through the VGG model and extract its features, which is the stored in a pockle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = image.load_img(path, target_size=(224,224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def img_encoding(model, i):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    image = load_image(i)\n",
    "    pred = model.predict(image)\n",
    "    pred = np.reshape(pred, pred.shape[1])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = {}\n",
    "for i in img:\n",
    "    encoded_images[i[len(DATASET):]] = img_encoding(encoding_model,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"encoded_images.pickle\", \"wb\")\n",
    "pickle.dump(encoded_images, f)\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
